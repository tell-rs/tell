# Tell Configuration
# ===================
#
# This is the configuration file for Tell.
# All settings have sensible defaults - only specify what you need to change.
#
# Minimal working config:
#
#   [[sources.tcp]]
#   port = 50000
#
#   [sinks.stdout]
#   type = "stdout"

# =============================================================================
# Global Settings
# =============================================================================
# These apply across all components and can be overridden per-component.

[global]
# Number of worker threads (default: number of CPU cores)
# num_processors = 8

# Default buffer size for network I/O (default: 262144 = 256KB)
# buffer_size = 262144

# Default batch size - messages per batch (default: 500)
# batch_size = 500

# Default channel queue size (default: 1000)
# queue_size = 1000

# Path to API keys file (default: "configs/apikeys.conf")
api_keys_file = "configs/apikeys.conf"

# =============================================================================
# Logging
# =============================================================================

[log]
# Log level: trace, debug, info, warn, error (default: info)
level = "info"

# Output format: console, json (default: console)
format = "console"

# Output destination: stdout, stderr (default: stdout)
output = "stdout"

# =============================================================================
# Metrics
# =============================================================================

[metrics]
# Enable metrics reporting (default: true)
enabled = true

# Reporting interval (default: 1h)
# For production monitoring, consider "10s" or "1m"
interval = "1h"

# Output format: human, json (default: human)
format = "human"

# Include pipeline metrics (default: true)
include_pipeline = true

# Include source metrics (default: true)
include_sources = true

# Include sink metrics (default: true)
include_sinks = true

# =============================================================================
# Sources
# =============================================================================
# Sources produce data that flows into the pipeline.
# Multiple instances of the same type are supported.

# -----------------------------------------------------------------------------
# TCP Sources - High-performance FlatBuffer protocol
# -----------------------------------------------------------------------------

# Primary TCP source for client traffic
[[sources.tcp]]
port = 50000
# enabled = true           # (default: true)
# address = "0.0.0.0"      # (default: "0.0.0.0")
# no_delay = true          # TCP_NODELAY (default: true)
# flush_interval = "100ms" # (default: 100ms)
# forwarding_mode = false  # Trust source_ip from upstream Tell instances (default: false)

# Secondary TCP source for edge Tell traffic (SOC forwarders)
# [[sources.tcp]]
# port = 8081
# forwarding_mode = true   # Trust source_ip from forwarders

# -----------------------------------------------------------------------------
# TCP Debug Source - Same as TCP but outputs hex dump for debugging
# -----------------------------------------------------------------------------
# Kept separate from main TCP to avoid performance impact.

# [sources.tcp_debug]
# port = 50001
# enabled = true

# -----------------------------------------------------------------------------
# Syslog Sources - RFC 3164 / RFC 5424
# -----------------------------------------------------------------------------
# Syslog sources don't use API key authentication - workspace_id is set in config.
# Messages are stored raw; parsing happens downstream in sinks/transformers.

# Syslog over TCP (reliable delivery, connection-oriented)
[sources.syslog_tcp]
enabled = false                    # Enable when needed
port = 1514                        # Non-privileged port (514 requires root)
workspace_id = "1"                 # Numeric workspace ID (like Go version)
# address = "0.0.0.0"              # (default: "0.0.0.0")
# max_message_size = 8192          # (default: 8192 = 8KB)
# connection_timeout = "30s"       # (default: 30s)
# no_delay = true                  # TCP_NODELAY (default: true)
# flush_interval = "100ms"         # (default: 100ms)

# Syslog over UDP (best-effort delivery, connectionless)
[sources.syslog_udp]
enabled = false                    # Enable when needed
port = 1514                        # Non-privileged port (514 requires root)
workspace_id = "1"                 # Numeric workspace ID
num_workers = 4                    # Parallel workers (default: 4)
# address = "0.0.0.0"              # (default: "0.0.0.0")
# max_message_size = 8192          # (default: 8192 = 8KB)

# =============================================================================
# Sinks
# =============================================================================
# Sinks consume data from the pipeline and write to destinations.
# Each sink has a unique name and type. The type field is required.

# -----------------------------------------------------------------------------
# Stdout Sink - Human-readable debug output
# -----------------------------------------------------------------------------

[sinks.stdout]
type = "stdout"
# enabled = true                   # (default: true)
# metrics_enabled = true           # Per-sink metrics (default: true)
# metrics_interval = "10s"         # (default: 10s)

# -----------------------------------------------------------------------------
# Null Sink - Discards all data (for benchmarking)
# -----------------------------------------------------------------------------

# [sinks.null]
# type = "null"

# -----------------------------------------------------------------------------
# Disk Plaintext Sink - Human-readable log files
# -----------------------------------------------------------------------------

[sinks.disk_plaintext]
type = "disk_plaintext"
path = "logs/"
# rotation = "daily"               # hourly, daily (default: daily)
# compression = "none"             # none, lz4 (default: none)
# buffer_size = 8388608            # (default: 8MB)
# write_queue_size = 10000         # (default: 10000)
# flush_interval = "100ms"         # (default: 100ms)
# metrics_enabled = true
# metrics_interval = "10s"

# -----------------------------------------------------------------------------
# Disk Binary Sink - High-performance binary storage
# -----------------------------------------------------------------------------

# [sinks.disk_binary]
# type = "disk_binary"
# path = "archive/"
# rotation = "hourly"              # hourly, daily (default: daily)
# compression = "lz4"              # none, lz4 (default: none)
# buffer_size = 33554432           # (default: 32MB)
# metrics_enabled = true
# metrics_interval = "10s"

# -----------------------------------------------------------------------------
# ClickHouse Sink - Real-time analytics database
# -----------------------------------------------------------------------------
# Two implementations available:
#   - clickhouse        Arrow HTTP format (recommended, high throughput)
#   - clickhouse_native Native protocol via clickhouse crate

# Arrow HTTP sink (recommended) - Uses HTTP port 8123
# Best for high throughput (65M+ events/sec), sends columnar Arrow batches
# [sinks.clickhouse]
# type = "clickhouse"
# host = "localhost:8123"          # HTTP port (required)
# database = "default"             # (default: "default")
# username = "default"             # (default: "default")
# password = ""                    # (default: "")
# hostname_prefix = "mycompany"    # Table prefix
# batch_size = 50000               # Rows per insert (default: 50000)
# flush_interval = "5s"            # (default: 5s)
# metrics_enabled = true
# metrics_interval = "10s"

# Native protocol sink - Uses native port 9000
# Use if you need native protocol features or have existing port 9000 setup
# [sinks.clickhouse_legacy]
# type = "clickhouse_native"
# host = "localhost:9000"          # Native port (required)
# database = "default"             # (default: "default")
# username = "default"             # (default: "default")
# password = ""                    # (default: "")
# hostname_prefix = "mycompany"    # Table prefix
# batch_size = 50000               # Rows per insert (default: 50000)
# flush_interval = "5s"            # (default: 5s)
# metrics_enabled = true
# metrics_interval = "10s"

# -----------------------------------------------------------------------------
# Parquet Sink - Columnar storage for data warehousing (cold data)
# -----------------------------------------------------------------------------
# Best for: archival, analytics queries, data warehousing, long-term storage
# Features: excellent compression, predicate pushdown, wide tool support

# [sinks.parquet]
# type = "parquet"
# path = "parquet/"
# rotation = "daily"               # hourly, daily (default: daily)
# data_format = "json"             # binary, json (default: json)
# compression = "snappy"           # snappy, gzip, lz4, uncompressed (default: snappy)
# metrics_enabled = true
# metrics_interval = "10s"

# -----------------------------------------------------------------------------
# Arrow IPC Sink - Fast columnar storage (hot data)
# -----------------------------------------------------------------------------
# Best for: hot data, real-time dashboards, inter-service communication
# Features: ~10x faster I/O than Parquet, zero-copy reads, no compression
#
# Files can be read with:
#   - DuckDB: SELECT * FROM 'path.arrow'
#   - PyArrow: pa.ipc.open_file("path.arrow")
#   - Polars: pl.read_ipc("path.arrow")

# [sinks.arrow_ipc]
# type = "arrow_ipc"
# path = "arrow/"
# rotation = "hourly"              # hourly, daily (default: hourly)
# buffer_size = 10000              # rows to buffer (default: 10000)
# flush_interval = "60s"           # (default: 60s)
# metrics_enabled = true
# metrics_interval = "10s"

# -----------------------------------------------------------------------------
# Forwarder Sink - Tell-to-Tell forwarding
# -----------------------------------------------------------------------------

# [sinks.forwarder]
# type = "forwarder"
# target = "tell.example.com:8081"  # Required when enabled
# api_key = "0123456789abcdef0123456789abcdef"  # 32 hex chars, required
# buffer_size = 262144             # (default: 256KB)
# connection_timeout = "10s"       # (default: 10s)
# write_timeout = "5s"             # (default: 5s)
# retry_attempts = 3               # (default: 3)
# retry_interval = "1s"            # (default: 1s)
# reconnect_interval = "5s"        # (default: 5s)

# =============================================================================
# Routing
# =============================================================================
# Defines how sources connect to sinks.
# Rules are evaluated in order; first match wins.
# Unmatched traffic goes to default sinks.
#
# Source names:
#   - tcp_main, tcp_1, tcp_2...  (TCP sources by index)
#   - syslog_tcp                  (Syslog TCP source)
#   - syslog_udp                  (Syslog UDP source)
#
# Source types:
#   - tcp      (matches all TCP sources)
#   - syslog   (matches syslog_tcp and syslog_udp)

[routing]
# Default sinks for unmatched traffic (empty = drop unmatched)
default = ["stdout"]

# Example routing rules (uncomment to use):

# Route all TCP traffic to disk and analytics
# [[routing.rules]]
# match = { source = "tcp_main" }
# sinks = ["disk_plaintext", "clickhouse"]

# Route debug TCP traffic to stdout only
# [[routing.rules]]
# match = { source = "tcp_debug" }
# sinks = ["stdout"]

# Route all syslog traffic (TCP and UDP) to disk
# [[routing.rules]]
# match = { source_type = "syslog" }
# sinks = ["disk_plaintext"]

# Route specific syslog source differently
# [[routing.rules]]
# match = { source = "syslog_tcp" }
# sinks = ["disk_plaintext", "stdout"]

# =============================================================================
# Transformers (per-route)
# =============================================================================
# Transformers process batches before routing to sinks.
# Each rule can have its own transformer chain with custom configuration.
#
# Available transformer types:
#   - noop            Pass-through (for testing)
#   - pattern_matcher Log pattern extraction using Drain algorithm
#
# Example: Route syslog with pattern extraction
# [[routing.rules]]
# match = { source_type = "syslog" }
# sinks = ["clickhouse"]
#
# [[routing.rules.transformers]]
# type = "pattern_matcher"
# enabled = true                      # (default: true)
# similarity_threshold = 0.5          # Pattern matching strictness 0.0-1.0 (default: 0.5)
# cache_size = 100000                 # L1 cache capacity (default: 100000)
# max_child_nodes = 100               # Drain tree branching factor (default: 100)
# persistence_enabled = false         # Enable file persistence (default: false)
# persistence_file = "/var/lib/tell/patterns.json"  # Pattern storage file
#
# Example: Multiple transformers in chain (processed in order)
# [[routing.rules]]
# match = { source = "tcp_main" }
# sinks = ["disk_plaintext"]
#
# [[routing.rules.transformers]]
# type = "pattern_matcher"
# similarity_threshold = 0.6
#
# [[routing.rules.transformers]]
# type = "noop"                       # Additional transformer in chain
#
# Example: Different configs for different routes (GDPR use case)
# High-sensitivity logs with aggressive pattern matching
# [[routing.rules]]
# match = { source = "sensitive_logs" }
# sinks = ["secure_storage"]
#
# [[routing.rules.transformers]]
# type = "pattern_matcher"
# similarity_threshold = 0.3          # More aggressive clustering
#
# Low-sensitivity logs with conservative pattern matching
# [[routing.rules]]
# match = { source = "general_logs" }
# sinks = ["analytics"]
#
# [[routing.rules.transformers]]
# type = "pattern_matcher"
# similarity_threshold = 0.7          # More conservative clustering

# =============================================================================
# API Client
# =============================================================================
# Configure CLI commands that connect to the Tell API server.
# Used by: tell auth login, tell auth status, etc.

[api]
url = "http://localhost:3000"

# =============================================================================
# Query
# =============================================================================
# Configure `tell query` command for SQL queries against your data.
# Supports ClickHouse (production) and local Arrow IPC files (Polars).
#
# Usage:
#   tell query "SELECT event_name, COUNT(*) FROM events GROUP BY event_name"
#   tell query "SELECT * FROM logs WHERE level = 'error' LIMIT 100" --format json

# Option 1: Reference a sink (recommended)
# Pulls host/database from the sink config. Override credentials for read-only access.
# [query]
# sink = "clickhouse"                  # References [sinks.clickhouse]
# username = "readonly_user"           # Override with read-only credentials (optional)
# password = "readonly_password"       # Falls back to sink credentials if not set
# workspace_id = 1                     # For local Arrow IPC file discovery

# Option 2: Inline configuration (no sink reference)
# [query]
# backend = "clickhouse"               # clickhouse, local
# url = "http://localhost:8123"
# database = "default"
# username = "default"
# password = ""
# workspace_id = 1

# Option 3: Local Arrow IPC files (via Polars)
# For development or edge deployments without ClickHouse
# [query]
# backend = "local"
# path = "arrow/"                      # Path to Arrow IPC files
# workspace_id = 1                     # Files at: {path}/{workspace_id}/**/*.arrow
